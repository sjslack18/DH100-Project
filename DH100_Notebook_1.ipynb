{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DH100 Notebook 1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjslack18/DH100-Project/blob/main/DH100_Notebook_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMKOKfm2H-em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c14feb-f792-4523-fc96-59bd5490f815"
      },
      "source": [
        "library(rvest)\n",
        "library(tidyverse)\n",
        "library(lubridate)\n",
        "\n",
        "\n",
        "#The following functions will be used to extract page IDs from pages on The APP\n",
        "# website containing a list of links, and to extract a \n",
        "#president's name from the title of the page.\n",
        "\n",
        "# function to extract text from hyperlink (as character)\n",
        "extract_link_title <- function(anchor_tag) {\n",
        "  return(unlist(strsplit(unlist(strsplit(anchor_tag, '>'))[2], '<'))[1])\n",
        "}\n",
        "# function to extract target URL from hyperlink (as character)\n",
        "extract_link_page_id <- function(anchor_tag) {\n",
        "  return(html_attr(\"href\"))\n",
        "}\n",
        "# function to extract president's name from title\n",
        "extract_president <- function(title) {\n",
        "  return(unlist(strsplit(title, ':'))[1])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in system(\"timedatectl\", intern = TRUE):\n",
            "“running command 'timedatectl' had status 1”\n",
            "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
            "\n",
            "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.3     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
            "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.2     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.6\n",
            "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
            "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.4.0     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
            "\n",
            "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m         masks \u001b[34mstats\u001b[39m::filter()\n",
            "\u001b[31m✖\u001b[39m \u001b[34mreadr\u001b[39m::\u001b[32mguess_encoding()\u001b[39m masks \u001b[34mrvest\u001b[39m::guess_encoding()\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m            masks \u001b[34mstats\u001b[39m::lag()\n",
            "\n",
            "\n",
            "Attaching package: ‘lubridate’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    date, intersect, setdiff, union\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kl-RWERakTc"
      },
      "source": [
        "#function for date access\n",
        "get_date <- function(page_data) {\n",
        "page_date <- page_data %>% html_nodes(xpath=\"//*[@id='block-system-main']/div/div/div[1]/div[2]/span\") %>%\n",
        "html_attr(\"content\") %>% as_datetime()\n",
        "return (page_date)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDKAC2Joqsbk"
      },
      "source": [
        "# The deglaze() function will scrape content from the site, given a page ID,\n",
        "# and do some minimal preprocessing (assemble title, date, and page text).\n",
        "\n",
        "deglaze <- function(page_id) {\n",
        "  page_data <- read_html(paste0('http://www.presidency.ucsb.edu', page_id))\n",
        "  page_title <- page_data %>%\n",
        "    html_node('title') %>%\n",
        "    html_text()\n",
        "  page_date <- page_data %>%\n",
        "    get_date()\n",
        "  page_text <- page_data %>%\n",
        "    html_nodes('p') %>%\n",
        "    html_text() %>%\n",
        "    paste(collapse = ' ')\n",
        "  return(as_tibble(cbind(title = page_title, text = page_text)) %>%\n",
        "   mutate(date = page_date))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2T4rVQSIjU"
      },
      "source": [
        "Taken from kshaffers github page and modified but largely unused, lots of problems encountered so I had to do most of it from scratch anyway. The first cell is mostly from kshaffer but is largely unused, and the deglaze function is his except for the page_date code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTmsGTQfKSH_"
      },
      "source": [
        "test_links <- read_html('https://www.presidency.ucsb.edu/documents/app-categories/pressmedia/press-briefings?items_per_page=60') %>%\n",
        "html_nodes('a') %>%\n",
        "html_attr(\"href\") %>%\n",
        "as_tibble() %>%\n",
        "filter(grepl('/documents/press',value,fixed=TRUE))\n",
        "test_links[1:5,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciDSVg0-SjXW"
      },
      "source": [
        "This cell tests a different process for gathering links to scrape, although I had difficulty finding a way to get titles from the original nodes that I wanted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHhRTw0d61au"
      },
      "source": [
        "# scraping one page for links\n",
        "\n",
        "#having difficulties with this, will try another way\n",
        "\n",
        "# pb_links <- read_html('https://www.presidency.ucsb.edu/documents/app-categories/pressmedia/press-briefings?items_per_page=60') %>%\n",
        "# html_nodes('a') %>%\n",
        "# as.character() %>%\n",
        "# as_tibble() %>%\n",
        "# unique() %>%\n",
        "# filter(grepl('/documents/press',value,fixed=TRUE)) %>%\n",
        "# mutate(title = mapply(extract_link_title, value),\n",
        "#          page_id = value) %>%\n",
        "#   select(title, page_id)\n",
        "# test = pb_links$title[1]\n",
        "# extract_link_page_id(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNXaMxqZSy8K"
      },
      "source": [
        "Unused for now, was having problems using the page_id because it seemed to be unusable for both regex and xml operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JHYensQShed"
      },
      "source": [
        "# # function to scrape all links from page into tibble (to be appended iteratively)\n",
        "# link_scraper <- function(page_address) {\n",
        "# link_tibble <- read_html(page_address) %>%\n",
        "# html_nodes('a') %>%\n",
        "# as.character() %>%\n",
        "# as_tibble() %>%\n",
        "# unique() %>%\n",
        "# filter(grepl('/documents/press-briefing',value,fixed=TRUE)) %>%\n",
        "# mutate(title = mapply(extract_link_title, value),\n",
        "#          page_id = mapply(extract_link_page_id, value)) %>%\n",
        "#   select(title, page_id)\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zvjxepaTDnI"
      },
      "source": [
        "link_scraper <- function(page_address) {\n",
        "  link_tibble <- read_html(page_address) %>%\n",
        "  html_nodes('a') %>%\n",
        "  html_attr(\"href\") %>%\n",
        "  as_tibble() %>%\n",
        "  filter(grepl('/documents/press',value,fixed=TRUE))\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pS2RHNSTEDo"
      },
      "source": [
        "New link scraper function based on test, returns only the link extensions and not titles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz7oSjnilbot"
      },
      "source": [
        "#compile all pages to scrape links for\n",
        "page_links <-  paste0('https://www.presidency.ucsb.edu/documents/app-categories/pressmedia/press-briefings?items_per_page=60&page=',c(1:105))\n",
        "\n",
        "#create tibble for first page (address has different format)\n",
        "pb_links <- link_scraper('https://www.presidency.ucsb.edu/documents/app-categories/pressmedia/press-briefings?items_per_page=60')\n",
        "\n",
        "#scrape all links iteratively, appending tibble after each iteration\n",
        "for (page in page_links) {\n",
        "new_links <- link_scraper(page)\n",
        "pb_links <- rbind(pb_links,new_links)\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnVzjBo4fDD6"
      },
      "source": [
        "#testing deglaze function\n",
        "test_page <- pb_links[367,1]\n",
        "test_results <- deglaze(test_page)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JttmpvJQ0uZU"
      },
      "source": [
        "#scrape all actual data in this step\n",
        "\n",
        "scraped_data <- deglaze(pb_links[1,1])\n",
        "for (row in c(2:nrow(pb_links))) {\n",
        "new_row <- deglaze(pb_links[[row,1]])\n",
        "scraped_data <- rbind(scraped_data, new_row)\n",
        "}\n",
        "\n",
        "scraped_data[1:5,]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVYB2bAzfZGA"
      },
      "source": [
        "Scraping and compiling the data into one very long tibble happens here: it take a long time to run this cell and seemingly is still problematic, though all the individual steps execute without problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uMYeNbgWdb"
      },
      "source": [
        "#Write to CSV\n",
        "head(scraped_data)\n",
        "write.csv(scraped_data, \"scraped_data.csv\", row.names = FALSE)\n",
        "# from google.colab import files\n",
        "# files.dowload(\"scraped_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPuRks4IrLf1"
      },
      "source": [
        "After writing the scraped data to a CSV, another notebook can be used, so that the previous cells will not have to run after the scraped data is uploaded to drive. This both cuts down on runtime by not rescraping and allows me to use python separately, which has better NLP processing tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rAxoLwLLCHi"
      },
      "source": [
        ""
      ]
    }
  ]
}